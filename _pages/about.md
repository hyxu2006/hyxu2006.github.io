---
permalink: /
title: "About Me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a Research Scientist at Core AI, [Meta Reality Labs](https://about.meta.com/realitylabs/?utm_source=about.facebook.com&utm_medium=redirect)
, where I build multimodal spatial vision systems for scalable video and world understanding. My research focuses on developing spatial intelligence models that integrate vision, language, motion, and geometry to enable robust perception, reasoning, and generation across modalities. Representative work includes [VideoAutoThink](https://arxiv.org/abs/2601.05175) (CVPR 2026), [VLM-3R](https://arxiv.org/pdf/2412.06974) (CVPR 2026 and Best Workshop Paper at ACM Multimedia 2025), and [MV-DUSt3R+]((https://github.com/facebookresearch/mvdust3r)) (CVPR 2025 Oral). In addition to research, I work on the on-device perception stack powering Meta Quest experiences, including semantic segmentation and real-time MR/VR spatial understanding. 

Before joining Meta Reality Labs, I was a technical lead and senior machine learning/computer vision engineer with the Video Engineering Group at Apple Inc. I have lead the algorithm development and delivered the shipments of multiple groundbreaking products, includes [Room Tracking](https://developer.apple.com/videos/play/wwdc2024/10100/?time=150) on VisionPro, [RoomPlan Enhancement](https://developer.apple.com/videos/play/wwdc2023/10192/) and [RoomPlan](https://developer.apple.com/augmented-reality/roomplan/). Additionally, I collaborated with Apple AIML on 3D Scene Style Generation, where we pioneered [RoomDreamer](https://machinelearning.apple.com/research/roomdreamer), the first paper to enable text-driven 3D indoor scene synthesis with coherent geometry and texture.

I received my Ph.D. and M.S. degree from University of Maryland, College Park, where I was advised by [Prof. Rama Chellappa](https://engineering.jhu.edu/faculty/rama-chellappa/). I completed my B.S. degree in Electrical Engineering and Information Science from [University of Science and Technology of China](http://en.ustc.edu.cn/). Additionally, I completed internships at Snap Research and the Palo Alto Research Center.

#  Highlights

* **Feb, 2026.** üöÄ‚ú® Three papers ‚Äî **1) VideoAutoThink**, **2) VLM-3R**, and **3) MoS (Mixture of States)** ‚Äî have been accepted to **CVPR 2026**! Huge thanks and congratulations to all co-authors and collaborators üôåüéâ
  * üß†üé¨ [**VideoAutoThink: Video Auto Reasoning via Thinking Once, Answering Twice**](https://arxiv.org/pdf/2601.05175)  
    An adaptive video reasoning framework that challenges unconditional chain-of-thought by adopting a *thinking-once, answering-twice* paradigm, enabling confidence-based reasoning activation for improved accuracy and efficiency.  
    üìÑ **Paper** [[arXiv]](https://arxiv.org/abs/2601.05175) ¬∑ üì¶ **GitHub** [[Code]](https://ivul-kaust.github.io/projects/videoauto-r1/) ¬∑ ü§ó **Model** [[Hugging Face]](https://huggingface.co/collections/IVUL-KAUST/videoauto-r1)
  * üß≠üìê [**VLM-3R: Instruction-Aligned 3D Reconstruction and Reasoning**](https://arxiv.org/pdf/2412.06974)  
    A spatial vision-language model aligning natural language instructions with 3D reasoning from monocular video.  
    üíª **GitHub** [[Code]](https://github.com/VITA-Group/VLM-3R) ¬∑ üì¶ **Project Page** [[Link]](https://vlm-3r.github.io)
  * üé®‚ö° [**Mixture of States: Routing Token-Level Dynamics for Multimodal Generation**](https://arxiv.org/pdf/2511.12207)  
    A dynamic token-wise routing mechanism for multimodal diffusion models, enabling adaptive layer selection and input-dependent text‚Äìvision alignment for scalable generation and editing.  
    üìÑ **Paper** [[arXiv]](https://arxiv.org/pdf/2511.12207)

* **Jan, 2026.** **VideoAuto-R1** is now online ‚Äî try our ü§ó **Demo** [[Hugging Face]](https://huggingface.co/spaces/sming256/VideoAuto-R1_Demo). üì¶ **GitHub** [[Code]](https://ivul-kaust.github.io/projects/videoauto-r1/) ¬∑ **Model** [[Hugging Face]](https://huggingface.co/collections/IVUL-KAUST/videoauto-r1)

* Dec, 2025. **MoS (Mixture of States)** is now online. Check out our [[Paper]](https://arxiv.org/pdf/2511.12207). Congrats to [Haozhe Liu](https://haozheliu-st.github.io/) and [Ding Liu](https://scholar.google.com/citations?user=PGtHUI0AAAAJ&hl=en) for leading the work.

* Nov, 2025. We‚Äôre grateful to receive the **Best Paper Award** from the ACM MM 2025 Multimodal Foundation Models for Spatial Intelligence Workshop. Congrats to [Zhiwen](https://zhiwenfan.github.io/) for leading the work, and thanks to all collaborators. [**üì¶ GitHub**](https://vlm-3r.github.io) and [**üìÑ Paper**](https://arxiv.org/pdf/2412.06974)

* Nov, 2025, **DynamicVerse**: Physically-Aware Multimodal Modeling for Dynamic 4D Worlds is accepted to NeurIPS 2025. [**üì¶ GitHub**]( https://dynamic-verse.github.io/)

* Jun, 2025. üöÄ **VLM-3R is online!** Check out our [**Project Website**](https://vlm-3r.github.io/), read the [**arXiv Paper**](https://arxiv.org/abs/2505.20279), and explore the [**Code**](https://github.com/VITA-Group/VLM-3R). 

* Mar, 2025, MV-DUSt3R+ is accepted as an **Oral** at CVPR 2025. Check our [**Demo**](https://www.youtube.com/watch?v=LBvnuKQ8Rso) and [**Project**](https://mv-dust3rp.github.io/). Congratulations to [Zhenggang Tang](https://recordmp3.github.io/), [Yuchen Fan](https://ychfan.github.io/), [Dilin Wang](https://wdilin.github.io/), Rakesh Ranjan, [Alexander Schwing](https://www.alexander-schwing.de/) and [Zhicheng Yan](https://sites.google.com/view/zhicheng-yan)

* Jan, 2025, MV-DUSt3R+ is [Open Souced](https://github.com/facebookresearch/mvdust3r). Let's further push the boundary!

* Dec, 2024. [MV-DUSt3R+](https://mv-dust3rp.github.io/) is online, a single-stage, multi-view, and multi-path model capable of reconstructing large-scale scenes from sparse, unconstrained views in just 2 seconds! 

* Jun, 2024. [Room Tracking](https://developer.apple.com/videos/play/wwdc2024/10100/?time=150) on [VisionPro](https://www.apple.com/apple-vision-pro/) is unveiled at Apple WWDC 2024. This technology identifies room boundaries, supports precisely-aligned geometries, and recognizes transitions between rooms.

* Oct, 2023. Our paper ‚ÄúRoomDreamer: Text-Driven 3D Indoor Scene Synthesis with Coherent Geometry and Texture‚Äù is accepted to ACM Multimedia 2023. [[arXiv]](https://arxiv.org/abs/2305.11337) [[demo]](https://www.youtube.com/watch?v=p4xgwj4QJcQ). Congratulations to [Liangchen Song](https://lsongx.github.io/), [Liangliang Cao](http://llcao.net/) and all co-authors.

* Jun, 2023. [RoomPlan Enhancement](https://developer.apple.com/videos/play/wwdc2023/10192/) is introduced at Apple WWDC 2023. It added numerous powerful features to RoomPlan, including multi-room scanning, multi-room layout, object attributes, polygon walls, improved furniture representation, room-type identification, and floor-shape recognition.

* Oct, 2022. Our research article, ‚Äú[3D Parametric Room Representation with RoomPlan](https://machinelearning.apple.com/research/roomplan)‚Äù is published at [Apple Machine Learning Research](https://machinelearning.apple.com/). Read our [research article](https://machinelearning.apple.com/research/roomplan) to learn more!

* Jun, 2022. [RoomPlan](https://developer.apple.com/videos/play/wwdc2022/10127/) is first released at Apple WWDC 2022. Combining the power of Apple LiDAR, state-of-the-art 3D machine learning, and an intuitive scanning UI, RoomPlan empowers developers to create innovative solutions in interior design, architecture, real estate, and e-commerce.