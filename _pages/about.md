---
permalink: /
title: "About Me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a Research Scientist at CoreAI, [Meta Reality Labs](https://about.meta.com/realitylabs/?utm_source=about.facebook.com&utm_medium=redirect), where I develop advanced on-device solutions to strengthen the perception stack for Meta‚Äôs MR/VR product lines. My research spans 3D reconstruction, generative AI, and 3D Vision-Language Models (3D VLMs), with a focus on bridging spatial understanding and multimodal intelligence.

Before joining Meta Reality Labs, I was a technical lead and senior machine learning/computer vision engineer with the Video Engineering Group at Apple Inc. I have lead the algorithm development and delivered the shipments of multiple groundbreaking products, includes [Room Tracking](https://developer.apple.com/videos/play/wwdc2024/10100/?time=150) on VisionPro, [RoomPlan Enhancement](https://developer.apple.com/videos/play/wwdc2023/10192/) and [RoomPlan](https://developer.apple.com/augmented-reality/roomplan/). Additionally, I collaborated with Apple AIML on 3D Scene Style Generation, where we pioneered [RoomDreamer](https://machinelearning.apple.com/research/roomdreamer), the first paper to enable text-driven 3D indoor scene synthesis with coherent geometry and texture.

I received my Ph.D. and M.S. degree from University of Maryland, College Park, where I was advised by [Prof. Rama Chellappa](https://engineering.jhu.edu/faculty/rama-chellappa/). I completed my B.S. degree in Electrical Engineering and Information Science from [University of Science and Technology of China](http://en.ustc.edu.cn/). Additionally, I completed internships at Snap Research and the Palo Alto Research Center. 

#  Highlights

* Jun, 2025. üöÄ **VLM-3R is online!** Check out our [**Project Website**](https://vlm-3r.github.io/), read the [**arXiv Paper**](https://arxiv.org/abs/2505.20279), and explore the [**Code**](https://github.com/VITA-Group/VLM-3R). **VLM-3R**, a Vision-Language Model designed for direct, end-to-end spatial and temporal reasoning from monocular video‚Äîno external sensors required. It seamlessly fuses 3D spatial context with language understanding, unlocking a new frontier in visual-spatial intelligence.

* Mar, 2025, MV-DUSt3R+ is accepted as an **Oral** at CVPR 2025. Check our [**Demo**](https://www.youtube.com/watch?v=LBvnuKQ8Rso) and [**Project**](https://mv-dust3rp.github.io/). Congratulations to [Zhenggang Tang](https://recordmp3.github.io/), [Yuchen Fan](https://ychfan.github.io/), [Dilin Wang](https://wdilin.github.io/), Rakesh Ranjan, [Alexander Schwing](https://www.alexander-schwing.de/) and [Zhicheng Yan](https://sites.google.com/view/zhicheng-yan)

* Jan, 2025, MV-DUSt3R+ is [Open Souced](https://github.com/facebookresearch/mvdust3r). Let's further push the boundary!

* Dec, 2024. [MV-DUSt3R+](https://mv-dust3rp.github.io/) is online, a single-stage, multi-view, and multi-path model capable of reconstructing large-scale scenes from sparse, unconstrained views in just 2 seconds! 

* Jun, 2024. [Room Tracking](https://developer.apple.com/videos/play/wwdc2024/10100/?time=150) on [VisionPro](https://www.apple.com/apple-vision-pro/) is unveiled at Apple WWDC 2024. This technology identifies room boundaries, supports precisely-aligned geometries, and recognizes transitions between rooms.

* Oct, 2023. Our paper ‚ÄúRoomDreamer: Text-Driven 3D Indoor Scene Synthesis with Coherent Geometry and Texture‚Äù is accepted to ACM Multimedia 2023. [[arXiv]](https://arxiv.org/abs/2305.11337) [[demo]](https://www.youtube.com/watch?v=p4xgwj4QJcQ). Congratulations to [Liangchen Song](https://lsongx.github.io/), [Liangliang Cao](http://llcao.net/) and all co-authors.

* Jun, 2023. [RoomPlan Enhancement](https://developer.apple.com/videos/play/wwdc2023/10192/) is introduced at Apple WWDC 2023. It added numerous powerful features to RoomPlan, including multi-room scanning, multi-room layout, object attributes, polygon walls, improved furniture representation, room-type identification, and floor-shape recognition.

* Oct, 2022. Our research article, ‚Äú[3D Parametric Room Representation with RoomPlan](https://machinelearning.apple.com/research/roomplan)‚Äù is published at [Apple Machine Learning Research](https://machinelearning.apple.com/). Read our [research article](https://machinelearning.apple.com/research/roomplan) to learn more!

* Jun, 2022. [RoomPlan](https://developer.apple.com/videos/play/wwdc2022/10127/) is first released at Apple WWDC 2022. Combining the power of Apple LiDAR, state-of-the-art 3D machine learning, and an intuitive scanning UI, RoomPlan empowers developers to create innovative solutions in interior design, architecture, real estate, and e-commerce.